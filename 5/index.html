<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <base href="./CS 180 Proj5 Results/data/">
  <title>CS 180 Project 5 - Diffusion Models (Parts A & B)</title>
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      background: #111;
      color: #eee;
      margin: 0;
      padding: 0 0 60px 0;
    }
    header {
      background: #222;
      padding: 20px 40px;
      position: sticky;
      top: 0;
      z-index: 10;
    }
    h1 {
      margin: 0 0 8px 0;
    }
    a {
      color: #88c9ff;
    }
    main {
      max-width: 1100px;
      margin: 0 auto;
      padding: 20px 20px 60px 20px;
    }
    section {
      margin-bottom: 50px;
      border-top: 1px solid #444;
      padding-top: 30px;
    }
    h2, h3, h4 {
      margin-top: 0;
    }
    .grid {
      display: flex;
      flex-wrap: wrap;
      gap: 16px;
      margin-top: 10px;
    }
    figure {
      margin: 0;
      text-align: center;
      font-size: 0.9rem;
    }
    figure img {
      display: block;
      width: 100%;
      height: auto;
      object-fit: contain;
      border-radius: 6px;
      border: 1px solid #444;
      background: #000;
    }
    .small img {
      max-width: 500px;
    }
    .caption-strong {
      font-weight: 600;
      margin-top: 4px;
    }
    nav ul {
      list-style: none;
      padding: 0;
      margin: 0;
      display: flex;
      flex-wrap: wrap;
      gap: 10px 20px;
      font-size: 0.9rem;
    }
    nav li::before {
      content: "• ";
      color: #888;
    }
    footer {
      text-align: center;
      font-size: 0.8rem;
      color: #aaa;
      margin-top: 40px;
    }
    @media (max-width: 700px) {
      figure img {
        width: 130px;
        height: 130px;
      }
    }
  </style>
</head>
<body>
  <header>
    <h1>CS 180 Project 5 - Diffusion Models (Parts A & B)</h1>
    <nav>
      <ul>
        <li><a href="#part0">Part 0: Setup / Text Prompts</a></li>
        <li><a href="#sec1-1">1.1 Forward Process</a></li>
        <li><a href="#sec1-2">1.2 Classical Denoising</a></li>
        <li><a href="#sec1-3">1.3 One-Step Denoising</a></li>
        <li><a href="#sec1-4">1.4 Iterative Denoising</a></li>
        <li><a href="#sec1-5">1.5 Sampling</a></li>
        <li><a href="#sec1-6">1.6 CFG</a></li>
        <li><a href="#sec1-7">1.7 Image-to-image / SDEdit</a></li>
        <li><a href="#sec1-71">1.7.1 Editing Hand-Drawn and Web Images</a></li>
        <li><a href="#sec1-72">1.7.2 Inpainting</a></li>
        <li><a href="#sec1-73">1.7.3 Text-Cond</a></li>
        <li><a href="#sec1-8">1.8 Visual Anagrams</a></li>
        <li><a href="#sec1-9">1.9 Hybrid Images</a></li>
        <li><a href="#partB">Part B: Flow Matching from Scratch</a></li>
        <li><a href="#b1-2">B1.2 Noising Visualization</a></li>
        <li><a href="#b1-21">B1.2.1 Train One-Step Denoiser</a></li>
        <li><a href="#b1-22">B1.2.2 OOD Noise</a></li>
        <li><a href="#b1-23">B1.2.3 Denoising Pure Noise</a></li>
        <li><a href="#b2-2">B2.2 Train Time-Cond UNet</a></li>
        <li><a href="#b2-3">B2.3 Sample Time-Cond UNet</a></li>
        <li><a href="#b2-5">B2.5 Train Class-Cond UNet</a></li>
        <li><a href="#b2-6">B2.6 Sample Class-Cond UNet</a></li>
        <li><a href="#b2-6-nosched">B2.6 No Scheduler</a></li>

      </ul>
    </nav>
  </header>

  <main>
    <!-- Part 0 -->
    <section id="part0">
      <h2>Part 0 - Playing with DeepFloyd IF</h2>
      <p>
        I generated prompt embeddings on the HuggingFace cluster and sampled
        images with DeepFloyd IF. Below are three of my favorite prompts and
        corresponding 100-step samples. I am using random seed = 100.

        Increasing the number of steps consistently made the images more coherent and closer to the text.
        The brain/canopy and heart-shaped city matched their prompts surprisingly well at high steps: 
        the global shapes (brain outline, heart outline) became clear while retaining a painter style. 
        The calculus-constellations prompt was harder—the sky looked good, but the “symbols” were only 
        loosely suggested rather than cleanly readable, showing that the model captures the general idea 
        but struggles with precise symbolic details. I tried 20, 40, and 100 for the num_inference_steps with distinct, progressive
        improvements.
      </p>
      <div class="grid">
        <figure>
          <img src="brain_canopy_100.png" alt="Watercolor brain &amp; tree canopy">
          <figcaption>
            <div class="caption-strong">Prompt:</div>
            a watercolor painting of a brain that also looks like a tree canopy
          </figcaption>
        </figure>
        <figure>
          <img src="heart_city_100.png" alt="Aerial view of city as heart">
          <figcaption>
            <div class="caption-strong">Prompt:</div>
            an aerial view of a city forming the shape of a human heart
          </figcaption>
        </figure>
        <figure>
          <img src="night_calculus_100.png" alt="Night sky calculus symbols">
          <figcaption>
            <div class="caption-strong">Prompt:</div>
            a starry night sky whose constellations form calculus symbols
          </figcaption>
        </figure>
      </div>
    </section>

    <!-- 1.1 -->
    <section id="sec1-1">
      <h2>1.1 Implementing the Forward Process</h2>
      <p>
        Here I apply the forward diffusion process to the Berkeley Campanile for
        different timesteps, adding progressively more Gaussian noise.
      </p>
      <div class="grid">
        <figure>
          <img src="campanile.png" alt="Original Campanile">
          <figcaption>Original Campanile</figcaption>
        </figure>
        <figure>
          <img src="campanilie_250.png" alt="Noisy t=250">
          <figcaption>Noisy Campanile at t=250</figcaption>
        </figure>
        <figure>
          <img src="campanile_500.png" alt="Noisy t=500">
          <figcaption>Noisy Campanile at t=500</figcaption>
        </figure>
        <figure>
          <img src="campanile_750.png" alt="Noisy t=750">
          <figcaption>Noisy Campanile at t=750</figcaption>
        </figure>
      </div>
    </section>

    <!-- 1.2 -->
    <section id="sec1-2">
      <h2>1.2 Classical Denoising</h2>
      <p>
        I used Gaussian blur as a classical denoising baseline on the three noisy
        Campanile images.
      </p>
      <div class="grid">
        <figure>
          <img src="campanilie_250.png" alt="Noisy 250">
          <figcaption>Noisy t=250</figcaption>
        </figure>
        <figure>
          <img src="gauss_denoised_250.png" alt="Gaussian denoised 250">
          <figcaption>Gaussian blur at t=250</figcaption>
        </figure>
        <figure>
          <img src="campanile_500.png" alt="Noisy 500">
          <figcaption>Noisy t=500</figcaption>
        </figure>
        <figure>
          <img src="gauss_denoised_500.png" alt="Gaussian denoised 500">
          <figcaption>Gaussian blur at t=500</figcaption>
        </figure>
        <figure>
          <img src="campanile_750.png" alt="Noisy 750">
          <figcaption>Noisy t=750</figcaption>
        </figure>
        <figure>
          <img src="gauss_denoised_750.png" alt="Gaussian denoised 750">
          <figcaption>Gaussian blur at t=750</figcaption>
        </figure>
      </div>
    </section>

    <!-- 1.3 -->
    <section id="sec1-3">
      <h2>1.3 One-Step Denoising with a Diffusion Model</h2>
      <p>
        Using the pretrained UNet denoiser, I estimate and remove the noise from
        each noisy Campanile in a single step.
      </p>
      <div class="grid">
        <figure>
          <img src="campanilie_250.png" alt="Noisy 250">
          <figcaption>Noisy t=250</figcaption>
        </figure>
        <figure>
          <img src="campanile_denoised_250.png" alt="Denoised 250">
          <figcaption>One-step estimate at t=250</figcaption>
        </figure>
        <figure>
          <img src="campanile_500.png" alt="Noisy 500">
          <figcaption>Noisy t=500</figcaption>
        </figure>
        <figure>
          <img src="campanile_denoised_500.png" alt="Denoised 500">
          <figcaption>One-step estimate at t=500</figcaption>
        </figure>
        <figure>
          <img src="campanile_750.png" alt="Noisy 750">
          <figcaption>Noisy t=750</figcaption>
        </figure>
        <figure>
          <img src="campanile_denoised_750.png" alt="Denoised 750">
          <figcaption>One-step estimate at t=750</figcaption>
        </figure>
      </div>
    </section>

    <section id="sec1-4">
      <h2>1.4 Iterative Denoising</h2>
      <p>
        Using <code>i_start = 10</code>, I create a strided schedule of timesteps and
        run the reverse diffusion loop. Below I show the noisy Campanile every 5th
        iteration and then compare three different ways of recovering the clean image.
      </p>
    
      <!-- Noisy snapshots every 5th loop -->
      <h4>Noisy Campanile during denoising (every 5th loop)</h4>
      <div class="grid small">
        <figure>
          <img src="campanile_iter_30.png" alt="Noisy Campanile at t=30">
          <figcaption>Noisy Campanile at t = 30</figcaption>
        </figure>
        <figure>
          <img src="campanile_iter_90.png" alt="Noisy Campanile at t=90">
          <figcaption>Noisy Campanile at t = 90</figcaption>
        </figure>
        <figure>
          <img src="campanile_iter_240.png" alt="Noisy Campanile at t=240">
          <figcaption>Noisy Campanile at t = 240</figcaption>
        </figure>
        <figure>
          <img src="campanile_iter_390.png" alt="Noisy Campanile at t=390">
          <figcaption>Noisy Campanile at t = 390</figcaption>
        </figure>
        <figure>
          <img src="campanile_iter_540.png" alt="Noisy Campanile at t=540">
          <figcaption>Noisy Campanile at t = 540</figcaption>
        </figure>
        <figure>
          <img src="campanile_iter_690.png" alt="Noisy Campanile at t=690">
          <figcaption>Noisy Campanile at t = 690</figcaption>
        </figure>
      </div>
    
      <!-- Final reconstructions -->
      <h4>Final reconstructions after denoising</h4>
      <div class="grid small">
        <figure>
          <img src="campanile.png" alt="Original Campanile">
          <figcaption>Original Campanile</figcaption>
        </figure>
        <figure>
          <img src="iterative_ddpm_campanile.png" alt="Iteratively denoised Campanile">
          <figcaption>Iteratively denoised Campanile</figcaption>
        </figure>
        <figure>
          <img src="one_step_campanile.png" alt="One-step denoised Campanile">
          <figcaption>One-step denoised Campanile</figcaption>
        </figure>
        <figure>
          <img src="blurred_campanile.png" alt="Gaussian blurred Campanile">
          <figcaption>Gaussian-blurred Campanile</figcaption>
        </figure>
      </div>
    </section>
    


    <!-- 1.5 -->
    <section id="sec1-5">
      <h2>1.5 Diffusion Model Sampling</h2>
      <p>
        Sampling from pure noise with the basic iterative denoiser (no CFG), using
        the prompt <em>"a high quality picture"</em>.
      </p>
      <div class="grid small">
        <figure>
          <img src="sample_1.png" alt="Sample 1">
          <figcaption>Sample 1</figcaption>
        </figure>
        <figure>
          <img src="sample_2.png" alt="Sample 2">
          <figcaption>Sample 2</figcaption>
        </figure>
        <figure>
          <img src="sample_3.png" alt="Sample 3">
          <figcaption>Sample 3</figcaption>
        </figure>
        <figure>
          <img src="sample_4.png" alt="Sample 4">
          <figcaption>Sample 4</figcaption>
        </figure>
        <figure>
          <img src="sample_5.png" alt="Sample 5">
          <figcaption>Sample 5</figcaption>
        </figure>
      </div>
    </section>

    <!-- 1.6 -->
    <section id="sec1-6">
      <h2>1.6 Classifier-Free Guidance (CFG)</h2>
      <p>
        Using CFG with scale γ = 7 noticeably sharpens and cleans up the sampled
        images for the same prompt <em>"a high quality picture"</em>.
      </p>
      <div class="grid small">
        <figure>
          <img src="sample_1_cfg.png" alt="Sample 1 CFG">
          <figcaption>Sample 1 with CFG</figcaption>
        </figure>
        <figure>
          <img src="sample_2_cfg.png" alt="Sample 2 CFG">
          <figcaption>Sample 2 with CFG</figcaption>
        </figure>
        <figure>
          <img src="sample_3_cfg.png" alt="Sample 3 CFG">
          <figcaption>Sample 3 with CFG</figcaption>
        </figure>
        <figure>
          <img src="sample_4_cfg.png" alt="Sample 4 CFG">
          <figcaption>Sample 4 with CFG</figcaption>
        </figure>
        <figure>
          <img src="sample_5_cfg.png" alt="Sample 5 CFG">
          <figcaption>Sample 5 with CFG</figcaption>
        </figure>
      </div>
    </section>

    <!-- 1.7 -->
    <section id="sec1-7">
      <h2>1.7 Image-to-image Translation (SDEdit)</h2>
      <p>
        I add noise to the real Campanile image and then denoise with CFG,
        gradually forcing the result back to the image manifold.
      </p>
      <div class="grid small">
        <figure>
          <img src="campanile_normal_prompt.png" alt="campanile with normal prompt">
          <figcaption>Campanile Denoising</figcaption>
        </figure>
        <figure>
          <img src="basketball_denoising.png" alt="basketball with normal prompt">
          <figcaption>Basketball Denoising</figcaption>
        </figure>
        <figure>
          <img src="math_denoising.png" alt="math with normal prompt">
          <figcaption>Math Denoising</figcaption>
        </figure>
      </div>
    </section>

    <!-- 1.7.1 -->
    <section id="sec1-71">
      <h3>1.7.1 Editing Hand-Drawn and Web Images</h3>
      <p>
        I apply the same procedure to both a web image and my own sketches.
      </p>
      <h4>Web Image</h4>
      <div class="grid small">
        <figure>
          <img src="duck.jpeg" alt="Original web duck">
          <figcaption>Original web duck image</figcaption>
        </figure>
        <figure>
          <img src="duckduck.png" alt="Edited web duck">
          <figcaption>Edited duck via SDEdit</figcaption>
        </figure>
      </div>

      <h4>Hand-drawn Image 1</h4>
      <div class="grid small">
        <figure>
          <img src="flag_draw.png" alt="Handdrawn ball & flag">
          <figcaption>Edited ball &amp; flag</figcaption>
        </figure>
        <figure>
          <img src="ball_with_flag.png" alt="Edited ball & flag">
          <figcaption>Original sketch: ball &amp; flag</figcaption>
        </figure>
      </div>

      <h4>Hand-drawn Image 2</h4>
      <div class="grid small">
        <figure>
          <img src="house_draw.png" alt="Original house sketch">
          <figcaption>Edited house sketch</figcaption>
        </figure>
        <figure>
          <img src="handdrawn_img.png" alt="Edited house sketch">
          <figcaption>Original house sketch</figcaption>
        </figure>
      </div>
    </section>

    <!-- 1.7.2 Inpainting -->
    <section id="sec1-72">
      <h2>1.7.2 Inpainting</h2>
      <p>
        Using a binary mask, I inpaint parts of the image while keeping the rest
        fixed according to the original.
      </p>

      <h3>Campanile Inpainting</h3>
      <div class="grid small">
        <figure>
          <img src="campanile.png" alt="Original Campanile">
          <figcaption>Original</figcaption>
        </figure>
        <figure>
          <img src="campanile_mask.png" alt="Campanile mask">
          <figcaption>Mask</figcaption>
        </figure>
        <figure>
          <img src="campanile_impainted.png" alt="Inpainted Campanile">
          <figcaption>Campanile inpainted</figcaption>
        </figure>
      </div>

      <h3>Own Image Inpainting - Sketch</h3>
      <div class="grid small">
        <figure>
          <img src="img_mask.png" alt="Mask for sketch">
          <figcaption>Mask over sketch</figcaption>
        </figure>
        <figure>
          <img src="img_inpainted.png" alt="Inpainted sketch">
          <figcaption>Inpainted sketch</figcaption>
        </figure>
      </div>

      <h3>Own Image Inpainting - Duck</h3>
      <div class="grid small">
        <figure>
          <img src="duck.jpeg" alt="Original yellow duck">
          <figcaption>Original duck</figcaption>
        </figure>
        <figure>
          <img src="duck_mask.png" alt="Duck mask">
          <figcaption>Duck mask</figcaption>
        </figure>
        <figure>
          <img src="duck_impainted.png" alt="Duck inpainted">
          <figcaption>Duck inpainted</figcaption>
        </figure>
      </div>
    </section>

    <!-- 1.7.3 -->
    <section id="sec1-73">
      <h2>1.7.3 Text-Conditioned Image-to-Image Translation</h2>
      <p>
        Now I guide SDEdit with a new text prompt. For the Campanile, I use the
        prompt <em>"a rocket ship"</em> with different noise levels, making the
        image look like a rocket while still resembling the tower. I did similar
        things for my two images.
      </p>
      <div class="grid small">
        <figure>
          <img src="rocket_campanile.png" alt="Rocket Campanile strip">
          <figcaption>Rocket ship-style edits at multiple noise levels</figcaption>
        </figure>
      </div>

      <h3>Text-conditioned edits for my own images</h3>
      <div class="grid small">
        <figure>
          <img src="rocket_duck.png" alt="Rocket duck">
          <figcaption>Prompt: a rocket ship</figcaption>
        </figure>
        <figure>
          <img src="rocket_draw.png" alt="Rocket over sketch">
          <figcaption>Prompt: a rocket ship</figcaption>
        </figure>
      </div>
    </section>

    <!-- 1.8 Visual Anagrams -->
    <section id="sec1-8">
      <h2>1.8 Visual Anagrams</h2>
      <p>
        For visual anagrams, I average noise estimates from two prompts: one for
        the upright orientation and another for the flipped image. The result is
        an image that changes meaning when rotated 180°.
      </p>

      <h3>Illusion 1 - Skull / Dove</h3>
      <div class="grid">
        <figure>
          <img src="skull_dove.png" alt="Skull and dove illusion">
          <figcaption>Upright: skull - upside down: white dove over canyon</figcaption>
        </figure>
      </div>

      <h3>Illusion 2 - Brain Canopy / City Skyline</h3>
      <div class="grid">
        <figure>
          <img src="braincanopy_city.png" alt="Brain canopy and city illusion">
          <figcaption>
            Upright: watercolor brain/tree canopy - flipped: city skyline / tower
          </figcaption>
        </figure>
      </div>
    </section>

    <!-- 1.9 Hybrid Images -->
    <section id="sec1-9">
      <h2>1.9 Hybrid Images</h2>
      <p>
        Finally, I combine low frequencies from one prompt with high frequencies
        from another using Gaussian blur, creating hybrid images that look
        different from near vs. far away.
      </p>

      <div class="grid ">
        <figure>
          <img src="brain_rocket.png" alt="Brain & rocket hybrid">
          <figcaption>Hybrid of brain and a rocket ship</figcaption>
        </figure>
        <figure>
          <img src="duck_croc.png" alt="Duck & croc hybrid">
          <figcaption>Hybrid of duck and croc sketch</figcaption>
        </figure>
      </div>
    </section>
    
    <!-- ===================== -->
    <!-- Part B: Flow Matching -->
    <!-- ===================== -->

    <section id="partB">
      <h2>Part B - Flow Matching from Scratch (MNIST)</h2>
      <p>
        In Part B, I trained UNet-based models on MNIST to (1) denoise images and (2) generate digits
        from pure noise using flow matching. I report training loss curves and qualitative samples at
        required epochs to track convergence and sample quality.
      </p>
    </section>

    <section id="b1-2">
      <h2>B1.2 Visualizing the Noising Process</h2>
      <p>
        To build intuition for diffusion-style corruption, I visualized how adding Gaussian noise
        changes an input image as the noise level increases. As noise increases, digit structure becomes
        less recognizable and eventually resembles pure noise.
      </p>
      <div class="grid small">
        <figure>
          <img src="sigmas.png" alt="Noising visualization over different noise levels">
          <figcaption>Noising process visualization (increasing noise)</figcaption>
        </figure>
      </div>
    </section>

    <section id="b1-21">
      <h2>B1.2.1 Training a One-Step Denoising UNet</h2>
      <p>
        I trained a UNet to map a noisy MNIST digit to its clean version using an L2 loss.
        During training, each batch is re-noised so the model generalizes to different noise realizations.
        Below are the training loss curve and denoised samples after epochs 1 and 5.
      </p>

      <h4>Training loss</h4>
      <div class="grid small">
        <figure>
          <img src="training_curve.png" alt="One-step denoiser training loss curve">
          <figcaption>One-step denoiser training loss</figcaption>
        </figure>
      </div>

      <h4>Test-set denoising results (noise level ≈ 0.5)</h4>
      <div class="grid small">
        <figure>
          <img src="test_epoch_1.png" alt="Denoising results after epoch 1">
          <figcaption>After Epoch 1</figcaption>
        </figure>
        <figure>
          <img src="training_epoch_5.png" alt="Denoising results after epoch 5">
          <figcaption>After Epoch 5</figcaption>
        </figure>
      </div>
    </section>

    <section id="b1-22">
      <h2>B1.2.2 Out-of-Distribution (OOD) Noise Testing</h2>
      <p>
        The one-step denoiser is trained under a specific noise distribution, so I tested robustness by
        evaluating the same digit under a range of noise levels. As noise moves out-of-distribution,
        performance typically degrades: the model either under-denoises (residual noise) or over-smooths
        structure.
      </p>
      <div class="grid small">
        <figure>
          <img src="ood_visual.png" alt="OOD denoising visualization across multiple noise levels">
          <figcaption>OOD denoising across different noise levels</figcaption>
        </figure>
      </div>
    </section>

    <section id="b1-23">
      <h2>B1.2.3 Denoising Pure Noise (Generative Attempt)</h2>
      <p>
        Here I trained the denoiser to map pure Gaussian noise directly to an MNIST-like output in a single step.
        With an MSE objective, the model is encouraged to predict an average/centroid-like target over many possible
        digits, which can produce blurry “prototypes” or mixtures of digit-like strokes rather than crisp, diverse samples.
      </p>

      <h4>Training loss</h4>
      <div class="grid small">
        <figure>
          <img src="pure_noise_curve.png" alt="Pure-noise denoiser training loss curve">
          <figcaption>Training loss for denoising pure noise</figcaption>
        </figure>
      </div>

      <h4>Samples after training</h4>
      <div class="grid small">
        <figure>
          <img src="pure_noise_1.png" alt="Pure-noise outputs after epoch 1">
          <figcaption>After Epoch 1</figcaption>
        </figure>
        <figure>
          <img src="pure_noise_5.png" alt="Pure-noise outputs after epoch 5">
          <figcaption>After Epoch 5</figcaption>
        </figure>
        <figure>
          <img src="samples_generated_from_noise.png" alt="Samples generated from random noise">
          <figcaption>Generated outputs from random noise (qualitative)</figcaption>
        </figure>
      </div>

      <p>
        <strong>Observed pattern:</strong> outputs tend to look like averaged digit strokes (shared shapes across classes),
        with limited diversity.<br/>
        <strong>Why:</strong> one-step denoising with an L2 loss tends to predict the conditional mean, which collapses multiple
        plausible digits into a single “best average” reconstruction rather than sampling multiple modes.
      </p>
    </section>

    <section id="b2-2">
      <h2>B2.2 Training a Time-Conditioned UNet (Flow Matching)</h2>
      <p>
        I trained a time-conditioned UNet to predict the flow from an interpolated noisy sample toward the clean data
        at randomly sampled timesteps. Conditioning on time teaches the model how to denoise progressively at different
        points along the trajectory.
      </p>
      <div class="grid small">
        <figure>
          <img src="time_cond_unet_traing_loss.png" alt="Time-conditioned UNet training loss curve">
          <figcaption>Time-conditioned UNet training loss</figcaption>
        </figure>
      </div>
    </section>

    <section id="b2-3">
      <h2>B2.3 Sampling from the Time-Conditioned UNet</h2>
      <p>
        Starting from pure noise, I iteratively update the sample using the predicted flow across timesteps.
        Over training, samples become increasingly digit-like; by later epochs the digit structure becomes clearer,
        though results are typically weaker than class-conditioned sampling.
      </p>
      <div class="grid small">
        <figure>
          <img src="time_cond_unet_1.png" alt="Time-conditioned samples after epoch 1">
          <figcaption>Epoch 1</figcaption>
        </figure>
        <figure>
          <img src="time_cond_unet_5.png" alt="Time-conditioned samples after epoch 5">
          <figcaption>Epoch 5</figcaption>
        </figure>
        <figure>
          <img src="time_cond_unet_10.png" alt="Time-conditioned samples after epoch 10">
          <figcaption>Epoch 10</figcaption>
        </figure>
      </div>
    </section>

    <section id="b2-5">
      <h2>B2.5 Training a Class-Conditioned UNet</h2>
      <p>
        I extended the model to condition on both time and digit class (one-hot vector), using classifier-free guidance
        during sampling by dropping the class conditioning with probability <code>p_uncond</code>.
        Class-conditioning improves convergence speed and sample fidelity because the model no longer needs to represent
        all digit modes simultaneously without guidance.
      </p>
      <div class="grid small">
        <figure>
          <img src="class_cond_unet_training_loss.png" alt="Class-conditioned UNet training loss curve (with scheduler)">
          <figcaption>Class-conditioned UNet training loss (with LR scheduler)</figcaption>
        </figure>
      </div>
    </section>

    <section id="b2-6">
      <h2>B2.6 Sampling from the Class-Conditioned UNet (CFG)</h2>
      <p>
        I sampled with class-conditioning and classifier-free guidance (γ = 5.0). Below are 4 instances per digit
        after epochs 1, 5, and 10. Compared to time-only conditioning, digits become legible faster and match their target
        classes more reliably.
      </p>
      <div class="grid small">
        <figure>
          <img src="class-cond_samples_w_lr_1.png" alt="Class-conditioned samples with scheduler after epoch 1">
          <figcaption>Epoch 1 (with scheduler)</figcaption>
        </figure>
        <figure>
          <img src="class-cond_samples_w_lr_5.png" alt="Class-conditioned samples with scheduler after epoch 5">
          <figcaption>Epoch 5 (with scheduler)</figcaption>
        </figure>
        <figure>
          <img src="class-cond_samples_w_lr_10.png" alt="Class-conditioned samples with scheduler after epoch 10">
          <figcaption>Epoch 10 (with scheduler)</figcaption>
        </figure>
      </div>
    </section>

    <section id="b2-6-nosched">
      <h2>B2.6 Removing the Learning Rate Scheduler</h2>
      <p>
        To simplify training, I removed the exponential LR scheduler and trained with a constant learning rate.
        Adam's adaptive updates helped keep training stable; the final sample quality remained comparable to the scheduler run.
        (If training ever became unstable, the simplest fix is lowering the constant LR slightly.)

        What I changed (to compensate for removing the scheduler):

        Lowered the learning rate from the scheduler setup to a fixed LR = 1e-3 (instead of starting
         at 1e-2 and decaying).
        This mimics the effect of a decayed learning rate by keeping updates small and stable 
        throughout training.

        Added gradient clipping:
        torch.nn.utils.clip_grad_norm_(fm_c.parameters(), 1.0)
        This prevents occasional large gradients from causing unstable jumps when there's no LR 
        decay to “cool down” training.

        Result:
        With constant LR + gradient clipping, training remained stable and the class-conditioned 
        samples after epochs 1, 5, and 10 stayed comparable in quality to the scheduler run, while 
        simplifying the training loop.
      </p>

      <h4>Training loss (no scheduler)</h4>
      <div class="grid small">
        <figure>
          <img src="class_cond_unet_training_loss_no_lr.png" alt="Class-conditioned UNet training loss curve (no scheduler)">
          <figcaption>Class-conditioned UNet training loss (no scheduler)</figcaption>
        </figure>
      </div>

      <h4>Sampling results (no scheduler)</h4>
      <div class="grid small">
        <figure>
          <img src="class_cond_unet_no_lr_1.png" alt="Class-conditioned samples without scheduler after epoch 1">
          <figcaption>Epoch 1 (no scheduler)</figcaption>
        </figure>
        <figure>
          <img src="class_cond_unet_no_lr_5.png" alt="Class-conditioned samples without scheduler after epoch 5">
          <figcaption>Epoch 5 (no scheduler)</figcaption>
        </figure>
        <figure>
          <img src="class_cond_unet_no_lr_10.png" alt="Class-conditioned samples without scheduler after epoch 10">
          <figcaption>Epoch 10 (no scheduler)</figcaption>
        </figure>
      </div>
    </section>


    <footer>
      CS 180 Project 5 - Parts A & B Results
    </footer>
  </main>

<script>
  document.addEventListener('DOMContentLoaded', () => {
    // Intercept all nav links that start with '#'
    document.querySelectorAll('nav a[href^="#"]').forEach(a => {
      a.addEventListener('click', (e) => {
        e.preventDefault();
        const id = a.getAttribute('href').substring(1);
        const el = document.getElementById(id);
        if (el) {
          el.scrollIntoView({ behavior: 'smooth', block: 'start' });
        }
      });
    });
  });
</script>  
</body>
</html>
